{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips\n",
    "root = '.'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models import *\n",
    "from renderer import *\n",
    "from data.ray_utils import get_rays\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "def read_depth(filename):\n",
    "    depth_h = np.array(read_pfm(filename)[0], dtype=np.float32) # (800, 800)\n",
    "    depth_h = cv2.resize(depth_h, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_NEAREST)  # (600, 800)\n",
    "    depth_h = depth_h[44:556, 80:720]  # (512, 640)\n",
    "#     depth = cv2.resize(depth_h, None, fx=0.5, fy=0.5,interpolation=cv2.INTER_NEAREST)#!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    mask = depth>0\n",
    "    return depth_h,mask\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llff no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([ 'fern']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir ../data/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff \\\n",
    "     --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar '\n",
    "\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_{scene}'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    rgbs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "                \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx\n",
    "            img_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "            positions = dataset.poses[img_idx,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset.poses[val_idx[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, img_feat, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad, lindisp=False)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples, lindisp=False)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                        near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test, lindisp=False)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                    xyz_NDC, z_vals, rays_o, rays_d,volume_feature,imgs_source, img_feat=None,**render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "\n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "#             img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "            \n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "    \n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['room']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir ../data/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff \\\n",
    "     --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar '\n",
    "\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            if 1!=i:\n",
    "                continue\n",
    "                \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx\n",
    "            img_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "            positions = dataset.poses[img_idx,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset.poses[val_idx[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = img_idx[:3]#[img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, img_feat, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad, lindisp=False)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples, lindisp=False)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                        near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test, lindisp=False)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                    xyz_NDC, z_vals, rays_o, rays_d,volume_feature,imgs_source, img_feat=None,**render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "\n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "#             img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "            \n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "    \n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fortress','flower','orchids', 'room','leaves','horns','trex','fern']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff \\\n",
    "     --ckpt ./ckpts/mvsnerf-v0.tar  \\\n",
    "     --net_type v0 --netwidth 128 --netdepth 6'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "        \n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['lego']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /home/amax_djh/code/ysl/data/nerf_synthetic/{scene} --dataset_name blender --white_bkgd --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/blender'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                intrinsic_ref[:2] *= args.imgScale_test/args.imgScale_train\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "#             img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['lego']):#'ship','mic','chair','lego','drums','ficus','materials','hotdog'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /home/amax_djh/code/ysl/data/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd \\\n",
    "     --ckpt /home/amax_djh/code/ysl/mvsnerf/runs_fine_tuning/lego-ft/ckpts/latest.tar \\\n",
    "     --net_type v2 --netwidth 128 --netdepth 6'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_blender_ft'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_depth_spiral.mp4', np.stack(depths_vis), fps=10, quality=10)\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "depth_acc = {}\n",
    "eval_metric = [0.1,0.05,0.01]\n",
    "depth_acc[f'abs_err'],depth_acc[f'acc_l_{eval_metric[0]}'],depth_acc[f'acc_l_{eval_metric[1]}'],depth_acc[f'acc_l_{eval_metric[2]}'] = {},{},{},{}\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,8,21,103,114\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft  \\\n",
    "     --net_type v0 --ckpt ./ckpts//mvsnerf-v0.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "\n",
    "            depth_gt, _ =  read_depth(f'/mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/Depths/scan{scene}/depth_map_{val_idx[i]:04d}.pfm')\n",
    "        \n",
    "            mask_gt = depth_gt>0\n",
    "            abs_err = abs_error(depth_rays_preds, depth_gt/200, mask_gt)\n",
    "\n",
    "            eval_metric = [0.01,0.05, 0.1]\n",
    "            depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "            depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "            depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "            depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "            \n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "        \n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "a = np.mean(list(depth_acc['abs_err'].values()))\n",
    "b = np.mean(list(depth_acc[f'acc_l_{eval_metric[0]}'].values()))\n",
    "c = np.mean(list(depth_acc[f'acc_l_{eval_metric[1]}'].values()))\n",
    "d = np.mean(list(depth_acc[f'acc_l_{eval_metric[2]}'].values()))\n",
    "print(f'============> abs_err: {a} <=================')\n",
    "print(f'============> acc_l_{eval_metric[0]}: {b} <=================')\n",
    "print(f'============> acc_l_{eval_metric[1]}: {c} <=================')\n",
    "print(f'============> acc_l_{eval_metric[2]}: {d} <=================')\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "    --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,torch\n",
    "import sys,os\n",
    "import numpy as np\n",
    "root = '/home/hengfei/Desktop/research/mvsnerf'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "pairs = torch.load('./configs/pairs.th')\n",
    "\n",
    "# llff\n",
    "root_dir = '/home/hengfei/Desktop/research/mvsnerf/xgaze/'\n",
    "for scene in ['xgaze_11images_cropped_colmapCODE']:#\n",
    "    poses_bounds = np.load(os.path.join(root_dir, scene, 'poses_bounds.npy'))  # (N_images, 11)\n",
    "    poses = poses_bounds[:, :15].reshape(-1, 3, 5)  # (N_images, 3, 5)\n",
    "    poses = np.concatenate([poses[..., 1:2], - poses[..., :1], poses[..., 2:4]], -1)\n",
    "\n",
    "    ref_position = np.mean(poses[..., 3],axis=0, keepdims=True)\n",
    "    dist = np.sum(np.abs(poses[..., 3] - ref_position), axis=-1)\n",
    "    pair_idx = np.argsort(dist)[:11]\n",
    "#     pair_idx = torch.randperm(len(poses))[:20].tolist()\n",
    "\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_train'] = np.delete(pair_idx, range(0,11,6))\n",
    "\n",
    "torch.save(pairs,'/home/hengfei/Desktop/research/mvsnerf/configs/pairs.th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantity evauation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips,cv2,torch,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)\n",
    "\n",
    "\n",
    "\n",
    "def acc_threshold(abs_err, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    acc_mask = abs_err < threshold\n",
    "    return  acc_mask.astype('float') if type(abs_err) is np.ndarray else acc_mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/new_disk_2/anpei/code/nerf/logs/'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.resize(cv2.imread(file)[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/scan{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file).astype('float')[...,::-1]\n",
    "        gt, img = img[:,:800]/255.0, img[:,800:1600]/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,:960].astype('float')/255.0, img[:,960:960*2].astype('float')/255.0\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu_scan{scene}_1h/dtu_scan{scene}_1h/00010239_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,640:1280]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibrnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "# psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "# for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "#     psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "#     files = sorted(glob.glob(f'{root}/nerf-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "#     for j, file in enumerate(files):\n",
    "\n",
    "#         idx = pairs[f'{scene}_val'][j]\n",
    "#         img = cv2.imread(file).astype('float')[...,::-1]\n",
    "#         gt, img = img[:,800:800*2]/255.0, img[:,800*3:800*4]/255.0\n",
    "\n",
    "# #         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "# #         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "# #         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "#         psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "#         ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "#         img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "#         img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "#         LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "#     print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "#     psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "# print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/IBRNet/logs/llff-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,1008:1008*2].astype('float')/255.0, img[:,1008*3:1008*4].astype('float')/255.0\n",
    "        img, gt = cv2.resize(img,(960,640)), cv2.resize(gt,(960,640))\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        \n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu-3view-finetuning-nearest-scan{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,3*640:4*640]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel nerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/pixel-nerf/visuals/dtu'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.resize(cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all,depth_acc = [],[],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'dtu_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        \n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        # depth\n",
    "#         depth_pred = torch.load(f'{root}/scan{scene}_{idx:03d}_depth.th')\n",
    "#         depth_gt,_ =  read_depth(f'/mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/Depths/scan{scene}/depth_map_{idx:04d}.pfm')\n",
    "        \n",
    "#         mask_gt = depth_gt>0\n",
    "#         abs_err = abs_error(depth_pred*1.5, depth_gt/200, mask_gt).numpy()\n",
    "\n",
    "#         eval_metric = [0.01,0.05, 0.1]\n",
    "#         depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#         depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# avt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['avt']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /home/amax_djh/code/ysl/data/avt_data_glass_20230204_7/ --dataset_name avt --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/avt'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds, dexdepth_rays_preds = [],[],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                intrinsic_ref[:2] *= args.imgScale_test/args.imgScale_train\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred, dexdepth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy(), extras['dexdepth_map'].cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "                dexdepth_rays_preds.append(dexdepth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            np.save(f'{save_dir}/{scene}_{val_idx[i]:03d}_depth.npy', depth_rays_preds)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "            dexdepth_rays_preds = np.concatenate(dexdepth_rays_preds).reshape(H, W)\n",
    "            np.save(f'{save_dir}/{scene}_{val_idx[i]:03d}_dexdepth.npy', dexdepth_rays_preds)\n",
    "            dexdepth_rays_preds, _ = visualize_depth_numpy(dexdepth_rays_preds, near_far_source)\n",
    "\n",
    "\n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds,dexdepth_rays_preds),axis=1)\n",
    "            \n",
    "#             img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avt ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['avt']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--dataset_name avt --datadir ../data/avt_data_glass_20230204_7 \\\n",
    "    --expname avt7-ft  --with_rgb_loss  --batch_size 2048  \\\n",
    "    --num_epochs 10000 --imgScale_test 1.0 --pad 0 \\\n",
    "    --ckpt ./ckpts/mvsnerf-v0.tar --N_vis 1 --use_density_volume --N_importance 128 --use_viewdirs --chunk 4096 --netchunk 4096 --raw_noise_std 1e0 --net_type v0 --ckpt ./runs_fine_tuning/avt7-ft_2/ckpts/latest.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+12\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_avt_ft'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds, dexdepth_rays_preds = [],[],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                intrinsic_ref[:2] *= args.imgScale_test/args.imgScale_train\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred, dexdepth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy(), extras['dexdepth_map'].cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "                dexdepth_rays_preds.append(dexdepth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            np.save(f'{save_dir}/{scene}_{val_idx[i]:03d}_depth.npy', depth_rays_preds)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "            dexdepth_rays_preds = np.concatenate(dexdepth_rays_preds).reshape(H, W)\n",
    "            np.save(f'{save_dir}/{scene}_{val_idx[i]:03d}_dexdepth.npy', dexdepth_rays_preds)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.imshow(dexdepth_rays_preds)\n",
    "            plt.show()\n",
    "            \n",
    "#             img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            # H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            # img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            # rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvsnerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "306d04d21b69bd882a1d22eb428f25bda44e2c1389b91b58737d0ecaa2730774"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
